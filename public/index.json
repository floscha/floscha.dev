
[{"content":" Introduction # The other day, I was wondering whether I could use AI to generate custom audio books based on existing documents like PDFs. While professionally recorded audiobooks already exist for most books, I wanted to have something more customizable where you could for example only use chapter summaries instead of the full chapter. Also, I wanted the audiobooks to be in the m4b format which comes as a single file but still supports chapters for easier navigation (unlike wav or mp3).\nTo achieve this, I had to solve the following parts:\nExtract relevant content from the source text. Split the content into chapters. Create audio for each chapter. Merge all chapters into a single audiobook. The final project can be found on GitHub: floscha/gemini-audiobook-generator\nThe goal of this post is not to walk you through the full implementation but rather discuss some things I found easier and some I found more challenging while developing this project.\nWhat Just Worked # Gemini API # To use Google\u0026rsquo;s Gemini AI models from Python code, you need a Gemini API key. Creating this key through Google AI Studio was really straight-forward and only required one or two clicks.\nSimilarly, writing the Python code to call the models went really effortlessly because of the great developer documentation which has Python snippets ready for many practical use cases. These include processing text from PDF files and converting text to audio which I needed. Because of this, I was done building the core AI logic within about 5 minutes.\nChallenges # Prompting # Writing the prompt to preprocess the source document into customized chapters turned out to be slightly more challenging than I initially thought.\nFor example, I use a simple heuristic where I take the first line of a chapter as the chapter name. Unfortunately, this does not work for some formatting like the one below.\nChapter 1 Name of the Chapter To make the heuristic work, I\u0026rsquo;ve added the following prompt:\nKeep the heading in a single line. This usually does the trick, but due to the probabilistic nature of LLMs, issues can still occur, for example dropping the Chapter 1 part of the example above.\nReusing Intermediate Files # While running the whole script from source might work for other tools, it is much more practical to store intermediate files from which the script can recover in case of failure or to speed up experimentation during development.\nEven though the Gemini free tier is really generous, a medium-sized PDF file can quickly result in 100k tokens per request, leading to higher than desirable token consumption. Creating the audio files on the other hand was not as token-expensive but took multiple minutes, leading to slower development speed.\nThe simple solution to this problem was to provide a keep_intermediate_files option which can be used during development. When turned on, intermediate files are kept and when re-running the script, it would recover from those files, rather than starting from scratch.\nMerging Audio Files # After the AI-specific code was implemented rather quickly, writing the code to merge the audio files for the individual chapters into a combined m4b file turned out to be much more tedious. This is mainly due to the fact that no Python library with this functionality exists.\nInstead, I had to fall back to using ffmpeg through subprocess calls. Even worse, this approach required generating lots of hacky metadata files through Python, which took me a couple of tries to get right.\nFinally, I see the lack of a Python m4b converter as an opportunity to create one myself which I might do in the near future.\n","date":"19 August 2025","externalUrl":null,"permalink":"/posts/building-an-audiobook-generator/","section":"Posts","summary":"","title":"Building an Audiobook Generator With Google Gemini","type":"posts"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/","section":"floscha.dev","summary":"","title":"floscha.dev","type":"page"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":" Introduction # Over the years, I have accumulated various scripts for smaller automations. One pretty trivial example is a script which takes the currently opened browser tab and stores the markdown link to this page (something like [Page Title](example.com)) in the clipboard.\nMost of these are written in Python but I neither want to spend time managing virtual environments nor global dependencies. Fortunately, uv scripts are an excellent tool for the job since they automatically create an virtual environment for the script but cache dependencies to not having to install all depencencies for every additional script.\nThe remaining question is how to launch those scripts. An obvious answer would be to use a terminal but opening an extra terminal window was too much friction for me (even when using Quake-like terminals such as Ghostty\u0026rsquo;s Quick Terminal). The perfect solution for me are Raycast\u0026rsquo;s Script Commands. Since I\u0026rsquo;m already using Raycast for many of its features, this didn\u0026rsquo;t require me to install additional software but instead allows me to run commands conveniently from a familiar interface.\nThis post will provide a full walkthrough on how to use uv and Raycast to launch arbitrary Python scripts using a command bar interface.\nPrerequisites # To use custom scripts with Raycast, all you have to do is tell it where those scripts are located. To do so, open Raycast Settings, Add New, Add Script Directory. Then, the directory on your machine where you want to store the files.\nCreating a new script # For new scripts, I always use the following template which covers all required boilerplate code and let\u0026rsquo;s you get straight to coding the actual functionality.\n#!/usr/bin/env -S uv run # # @raycast.schemaVersion 1 # @raycast.title Copy Markdown Link # @raycast.mode silent # # /// script # dependencies = [ # \u0026#34;some-dependency==1.0.0\u0026#34; # ] # /// # TODO: Put your code here! The first line makes it a uv script and lets you execute it with ./my_script.py instead of requiring to add python or uv run (make sure to adjust the permissions with chmod u+x my_script).\nNext, the comments with @raycast add the required metadata for the script to be run as a Raycast script:\n@raycast.schemaVersion adds the schema version and for the time being should always be 1. @raycast.title sets the title to be displayed in Raycast. @raycast.mode defines how the output of the script is displayed in Raycast. For details see Output Modes. Then, the /// script block is used by uv to define dependencies and the required Python version can also be set here. While exact versions for dependencies are optional, they are recommended to make the script more reproducible independent of what systems it is run on.\nBonus tip: Create a Raycast Snippet for the template to quickly create future scripts.\nUsing the script # When saving the script to the earlier defined script directory, Raycast will automatically find it by searching for the text you\u0026rsquo;ve used for @raycast.title. You can then also find it in Settings under Extensions where you can assign a hotkey to the script.\nFull Example # The following code shows the full implementation of the markdown link script I\u0026rsquo;ve mentioned in the introduction.\n#!/usr/bin/env -S uv run # # @raycast.schemaVersion 1 # @raycast.title Copy Markdown Link # @raycast.mode silent # # /// script # dependencies = [ # \u0026#34;sh\u0026#34;, # ] # /// from sh import osascript if __name__ == \u0026#34;__main__\u0026#34;: page_title = osascript(e=\u0026#39;tell application \u0026#34;Google Chrome\u0026#34; to get title of active tab of first window\u0026#39;).strip() page_url = osascript(e=\u0026#39;tell application \u0026#34;Google Chrome\u0026#34; to get URL of active tab of first window\u0026#39;).strip() markdown_link = f\u0026#34;[{page_title}]({page_url})\u0026#34; osascript(e=f\u0026#39;set the clipboard to \u0026#34;{markdown_link}\u0026#34;\u0026#39;) print(f\u0026#34;ðŸ“‹ Successfully copied {markdown_link} to clipboard.\u0026#34;) ","date":"17 August 2025","externalUrl":null,"permalink":"/posts/uv-scripts-with-raycast/","section":"Posts","summary":"","title":"Using uv Scripts with Raycast","type":"posts"},{"content":" Introduction # This post describes how I use several AI tools to help me study topics of any kind.\nFor this, I usually go through the following 3 steps:\nAssemble the learning material with GitHub Copilot Create cards with AI Studio Create audio summary with NotebookLM and AI Studio Assemble Learning Material # First, I collect all the study material I already have. For topics where I don\u0026rsquo;t have any material or just need a lot more, I use an agentic tool like GitHub Copilot, which I am most comfortable with as a software engineer. Also, running the agent locally has the benefit that you can easily split your material across multiple files instead of bundling it in a large response. In the future, however, I should also try this approach with other agentic tools like ChatGPT Deep Research.\nI then pass the agent a prompt similar to this:\nBased on the topics and subtopics represented as topics below, for each bullet, create a comprehensive section, explaining all related concepts in detail. Take your information from \u0026lt;some-related-website\u0026gt; and all sub pages. Create a markdown file for each topic. Place the markdown files in a \u0026lt;name-of-the-subject\u0026gt; folder. This leaves you with an organized number of markdown files which can already be used for studying by themselves.\nCreate Cards # Next, we use the markdown files to create flashcards for the Mochi app, which lets you easily import cards for studying using spaced repetition.\nThis can be achieved with the same local agent as above using this prompt:\nFor each topic markdown file, create flashcards in csv format, where the first column is the question and the second column the answer. Do not use headers. These CSV files can then be imported in Mochi using the Import menu and the CSV option. Make sure to set Quote to \u0026quot;.\nCreate Audio Summary # Create a new notebook for each topic in NotebookLM and either upload the markdown file or copy its content via Paste text \u0026gt; Copied text. Then, click on Audio Overview to create a podcast based on the imported study material.\nWhile the interaction of the two virtual podcast hosts can be entertaining, I often prefer a more concise single-speaker audio summary, which we can obtain using the following steps (inspired by Tina Huang):\nDownload the podcast audio file in the playback section of NotebookLM. Import the audio file in a tool like AI Studio. Transcribe it using a prompt like Accurately transcribe the uploaded podcast with two speakers. Do not use timestamps. Convert this transcript into a single host format and remove all extraneous information and commentary. Stay concise but do not remove any facts. Write it such that it can be read by a single speaker. Copy the output. Go to Generate Media \u0026gt; Generate speech and paste the output from the previous step. Wait until the generation has finished and either listen in AI Studio or download the audio to listen offline. ","date":"15 August 2025","externalUrl":null,"permalink":"/posts/study-with-ai/","section":"Posts","summary":"","title":"How I Study With AI","type":"posts"},{"content":" Introduction # As a fan of make-like tools such as just or Task and appreciating the guardrails provided by Git pre-commit, I dislike the redundancy of having to define commands like linting both in the justfile/taskfile as well as in the .pre-commit-config.yaml. To solve this, I\u0026rsquo;ve built a simple Git-native pre-commit hook which only delegates to a just recipe (in our case called lint) where all the checks are defined. While the following examples use just, they are easily adaptable to Task or even good old make.\nDefine just Recipe # As a prerequisite, we need a justfile which contains all the checks we expect to pass (meaning the commands results in a 0 return code), before a new Git commit can be created. For a Python codebase this could look like this:\nlint: just --fmt --unstable --check uvx ruff check uvx ruff format --check Set Up Pre-commit Hook # Within your repository, create a new file for the pre-commit hook using the editor of your choice. For VS Code, run\ntouch .git/hooks/pre-commit The pre-commit file should then have the following content.\n#!/bin/sh # Prevent commit if \u0026#39;just lint\u0026#39; recipe fails. if ! just lint; then echo \u0026#34;Pre-commit check failed!\u0026#34; exit 1 fi Finally, set the permissions such that the hook can be executed.\nchmod u+x .git/hooks/pre-commit Enable/Disable Hook # To disable the git hook, remove the permission to execute the script.\nchmod u-x .git/hooks/pre-commit Consequently, to re-enable the hook, add the permission again.\nchmod u+x .git/hooks/pre-commit Bonus: Putting Everything in a justfile # To not set up the pre-commit hook manually, you can also do this via a enable-pre-commit recipe.\nlint: just --fmt --unstable --check uvx ruff check uvx ruff format --check @enable-pre-commit: echo \u0026#39;#!/bin/sh\\n\\n# Prevent commit if \u0026#39;\\\u0026#39;\u0026#39;just lint\u0026#39;\\\u0026#39;\u0026#39; recipe fails.\\nif ! just lint; then\\n echo \u0026#34;Pre-commit check failed!\u0026#34;\\n exit 1\\nfi\u0026#39; \u0026gt; .git/hooks/pre-commit chmod u+x .git/hooks/pre-commit @echo \u0026#34;Pre-commit hook enabled.\u0026#34; @disable-pre-commit: chmod u-x .git/hooks/pre-commit @echo \u0026#34;Pre-commit hook disabled.\u0026#34; ","date":"14 August 2025","externalUrl":null,"permalink":"/posts/use-just-as-a-git-pre-commit-hook/","section":"Posts","summary":"","title":"Use just as a Git Pre-Commit Hook","type":"posts"},{"content":" Introduction # I\u0026rsquo;ve just switched my Hugo theme from PaperMod to PaperModX for the features listed on their repository. The steps to do so are pretty straightforward and listed below.\nMigration Steps # First, add the theme as a sub-module.\ngit submodule add https://github.com/reorx/hugo-PaperModX.git themes/PaperModX Then, change the Hugo theme to PaperModX.\nsed -i \u0026#39;\u0026#39; \u0026#39;s/^theme: PaperMod$/theme: PaperModX/\u0026#39; hugo.yaml Finally, delete the now unused PaperMod sub-module.\ngit rm themes/PaperMod ","date":"13 August 2025","externalUrl":null,"permalink":"/posts/migrate-to-papermodx/","section":"Posts","summary":"","title":"Migrate Hugo From PaperMod Theme to PaperModX","type":"posts"},{"content":" Introduction # While the Copilot team has taken measures to prohibit the agent from executing disastrous commands, auto approving commands will always hold an inherent risk.\nAuto Approve Everything # To auto approve all commands the agent intends to run, follow the steps below.\nOpen the VS Code setting with cmd + , In the Search settings text field, type chat auto approve Click the check box to enable the setting Again, in Search settings, type chat max requests For me, this setting was set at a default of 25 which was not sufficient for more sophisticated tasks, so I increased this value to 100 Auto Approve Selected Commands # As described in this GitHub issue, the Copilot agent now supports explicit enabling and disabling of commands:\n\u0026#34;chat.agent.terminal.autoApprove\u0026#34;: { \u0026#34;foo\u0026#34;: true, \u0026#34;bar\u0026#34;: false } ","date":"12 August 2025","externalUrl":null,"permalink":"/posts/github-copilot-auto-approve/","section":"Posts","summary":"","title":"Github Copilot Auto Approve","type":"posts"},{"content":" Introduction # The goal of today\u0026rsquo;s post is to dig into the Python\u0026rsquo;s functools.partial object.\nPartial Basics # Let\u0026rsquo;s start by importing partial from Python\u0026rsquo;s built-in functools.\n\u0026gt;\u0026gt;\u0026gt; from functools import partial Next, we define a simplistic multiply method and a times_two partial object which fixes the multiplier to always be 2.\n\u0026gt;\u0026gt;\u0026gt; def multiply(a: int, b: int) -\u0026gt; int: ... return a * b \u0026gt;\u0026gt;\u0026gt; times_two = partial(multiply, b=2) \u0026gt;\u0026gt;\u0026gt; times_two(3) 6 When inspecting the __name__ and __doc__ fields that regular Python methods always provide, we notice that for partial objects __name__ does not exists and __doc__ is set to some generic partial object default. So to have sensible values for the two fields, we have to set those ourselves.\n\u0026gt;\u0026gt;\u0026gt; times_two.__name__ Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; AttributeError: \u0026#39;functools.partial\u0026#39; object has no attribute \u0026#39;__name__\u0026#39;. Did you mean: \u0026#39;__ne__\u0026#39;? \u0026gt;\u0026gt;\u0026gt; times_two.__name__ = \u0026#39;times_two\u0026#39; \u0026gt;\u0026gt;\u0026gt; times_two.__name__ \u0026#39;times_two\u0026#39; \u0026gt;\u0026gt;\u0026gt; times_two.__doc__ \u0026#39;partial(func, *args, **keywords) - new function with partial application\\n of the given arguments and keywords.\\n\u0026#39; \u0026gt;\u0026gt;\u0026gt; times_two.__doc__ = \u0026#39;Multiply the given value times two.\u0026#39; \u0026gt;\u0026gt;\u0026gt; times_two.__doc__ \u0026#39;Multiply the given value times two.\u0026#39; When printing our times_two partial we get the name of the class together with a reference to the original method and the arguments that we\u0026rsquo;ve fixed for our partial.\n\u0026gt;\u0026gt;\u0026gt; times_two functools.partial(\u0026lt;function multiply at 0x101358900\u0026gt;, b=2) We can also get those latter two explicitly, using the .func and keywords fields of partial.\n\u0026gt;\u0026gt;\u0026gt; times_two.func \u0026lt;function multiply at 0x101358900\u0026gt; \u0026gt;\u0026gt;\u0026gt; times_two.keywords {\u0026#39;b\u0026#39;: 2} If we had used partial with non-keyword arguments (like partial(multiply, 2), which would\u0026rsquo;ve set a to 2), those values would show up in the .args field. Since we\u0026rsquo;ve only used keyword arguments however, args will be an empty tuple in our case.\n\u0026gt;\u0026gt;\u0026gt; times_two.args () Partial With Types # If we ask Python for the type of the partial, it will simply tell us it\u0026rsquo;s a functools.partial object.\n\u0026gt;\u0026gt;\u0026gt; type(times_two) \u0026lt;class \u0026#39;functools.partial\u0026#39;\u0026gt; And while we can get the argument names with their types as well the return type from a regular Python method using \u0026lt;method_name\u0026gt;.__annotations__, this dunder method is not provided for partial objects.\n\u0026gt;\u0026gt;\u0026gt; times_two.__annotations__ Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; AttributeError: \u0026#39;functools.partial\u0026#39; object has no attribute \u0026#39;__annotations__\u0026#39; However, we can retrieve the same output using the simple dict comprehension below.\n\u0026gt;\u0026gt;\u0026gt; {k: v for k, v in times_two.func.__annotations__.items() if k not in times_two.keywords} {\u0026#39;a\u0026#39;: \u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, \u0026#39;return\u0026#39;: \u0026lt;class \u0026#39;int\u0026#39;\u0026gt;} ","date":"11 August 2025","externalUrl":null,"permalink":"/posts/exploring-functools-partial/","section":"Posts","summary":"","title":"Exploring functools.partial","type":"posts"},{"content":"Hello world, hej vÃ¤rlden!\n","date":"10 August 2025","externalUrl":null,"permalink":"/posts/hello-world/","section":"Posts","summary":"","title":"Hello World","type":"posts"},{"content":"Some things about me\u0026hellip;\nCertifications # Some of the certifications, I\u0026rsquo;ve acquired recently in the fields of data engineering and solution architecture.\nAWS Certified Solutions Architect â€“ Professional AWS Certified Data Engineer â€“ Associate Databricks Certified Data Engineer Professional [dbt Engineer] [dbt Architect] ","externalUrl":null,"permalink":"/about/","section":"floscha.dev","summary":"","title":"About me","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]